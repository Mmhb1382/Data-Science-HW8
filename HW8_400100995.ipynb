{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e279eb6a5e7161b7",
   "metadata": {},
   "source": [
    "# HW8 on Data Science course of Sharif University of Technology\n",
    "## Created by: Mohammad Mahdi Hossein Beiky     SI: 400100995\n",
    "## GitHub URL: https://github.com/Mmhb1382/Data_Science_HW8.git\n",
    "---"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T14:51:10.920660Z",
     "start_time": "2025-04-26T14:51:09.594480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hw8_task1_svm.py\n",
    "\n",
    "\"\"\"\n",
    "Task 1: Multiclass SVM on the Ames Housing Dataset\n",
    "\n",
    "Steps:\n",
    " 1. Load the raw CSV into a pandas DataFrame.\n",
    " 2. Create a 4‐class target ('PriceClass') by quartiling SalePrice.\n",
    " 3. Build preprocessing pipelines to median‐impute & scale numerics, and to fill/one‐hot encode categoricals.\n",
    " 4. Apply the pipelines and assemble `final_df` (features + 'PriceClass').\n",
    " 5. Split `final_df` into train/test sets (80/20), stratified on 'PriceClass'.\n",
    " 6. Fit a LinearSVC on the preprocessed features.\n",
    " 7. Evaluate with macro‐F1 and output a full classification report.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Load the dataset\n",
    "df = pd.read_csv('AmesHousing.csv')  # adjust path if needed\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Create the multiclass target\n",
    "#    Split SalePrice into 4 equal‐size bins labeled 0,1,2,3\n",
    "df['PriceClass'] = pd.qcut(df['SalePrice'], q=4, labels=False)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Prepare data for preprocessing\n",
    "X_raw = df.drop(['SalePrice', 'PriceClass'], axis=1)\n",
    "y     = df['PriceClass']\n",
    "\n",
    "# Identify numeric vs. categorical columns\n",
    "numeric_features     = X_raw.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_raw.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Build preprocessing pipelines\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # fill gaps with median\n",
    "    ('scaler',   StandardScaler()),                  # standardize features\n",
    "])\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot',  OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline,     numeric_features),\n",
    "    ('cat', categorical_pipeline, categorical_features),\n",
    "])\n",
    "\n",
    "# Apply preprocessing to raw features\n",
    "X_processed = preprocessor.fit_transform(X_raw)\n",
    "\n",
    "# Retrieve feature names for the one‐hot columns\n",
    "ohe       = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "cat_names = ohe.get_feature_names_out(categorical_features)\n",
    "feature_names = numeric_features + list(cat_names)\n",
    "\n",
    "# Build final_df containing all processed features\n",
    "final_df = pd.DataFrame(X_processed, columns=feature_names, index=X_raw.index)\n",
    "\n",
    "# Attach the multiclass target to final_df\n",
    "final_df['PriceClass'] = y\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Train/test split (stratified by PriceClass)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    final_df.drop('PriceClass', axis=1),\n",
    "    final_df['PriceClass'],\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=final_df['PriceClass']\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 6) Train a Linear SVM\n",
    "svm = LinearSVC(max_iter=10000, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 7) Evaluate performance\n",
    "y_pred   = svm.predict(X_test)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"✅ Macro F1‐score: {macro_f1:.3f}  (goal ≥ 0.625)\\n\")\n",
    "print(\"Full classification report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "id": "1ef0dc5857d27dd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Macro F1‐score: 0.753  (goal ≥ 0.625)\n",
      "\n",
      "Full classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82       148\n",
      "           1       0.67      0.62      0.65       146\n",
      "           2       0.70      0.66      0.68       146\n",
      "           3       0.83      0.88      0.86       146\n",
      "\n",
      "    accuracy                           0.76       586\n",
      "   macro avg       0.75      0.76      0.75       586\n",
      "weighted avg       0.75      0.76      0.75       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T14:56:04.854287Z",
     "start_time": "2025-04-26T14:56:04.467542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Task 2.1 — Logistic Regression (OVR)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# 1) Build a base LogisticRegression (no multi_class argument)\n",
    "base_lr = LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2) Wrap it in OneVsRestClassifier to get OVR behavior\n",
    "logreg_ovr = OneVsRestClassifier(base_lr)\n",
    "\n",
    "# 3) Fit on the preprocessed training data\n",
    "logreg_ovr.fit(X_train, y_train)\n",
    "\n",
    "# 4) Predict & evaluate\n",
    "y_pred_ovr = logreg_ovr.predict(X_test)\n",
    "f1_ovr     = f1_score(y_test, y_pred_ovr, average='macro')\n",
    "\n",
    "print(f\"✅ OVR Logistic Regression macro-F1: {f1_ovr:.3f}  (goal ≥ 0.625)\\n\")\n",
    "print(\"Classification Report (OVR):\\n\", classification_report(y_test, y_pred_ovr))\n"
   ],
   "id": "8c3b337af5f2c450",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OVR Logistic Regression macro-F1: 0.764  (goal ≥ 0.625)\n",
      "\n",
      "Classification Report (OVR):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.82       148\n",
      "           1       0.67      0.61      0.64       146\n",
      "           2       0.74      0.68      0.71       146\n",
      "           3       0.86      0.92      0.89       146\n",
      "\n",
      "    accuracy                           0.77       586\n",
      "   macro avg       0.76      0.77      0.76       586\n",
      "weighted avg       0.76      0.77      0.76       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:14:50.122213Z",
     "start_time": "2025-04-26T15:11:28.540763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Task 2.2 — Tuned Multinomial Logistic Regression (no warning, higher F1)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Base LR (solver='saga' supports both l1 & l2 penalties)\n",
    "base_lr = LogisticRegression(\n",
    "    solver='saga',\n",
    "    max_iter=20000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Grid over penalty type, regularization C, and class weights\n",
    "param_grid = {\n",
    "    'penalty':      ['l2'],               # drop 'l1' for now\n",
    "    'C':            [0.1, 1, 10],         # fewer values\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    base_lr,\n",
    "    param_grid,\n",
    "    cv=3,                 # 3-fold instead of 5-fold\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Pull out the best model\n",
    "best_lr  = grid.best_estimator_\n",
    "best_params = grid.best_params_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred   = best_lr.predict(X_test)\n",
    "f1_best  = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"🏆 Best parameters: {best_params}\")\n",
    "print(f\"✅ Tuned Logistic Regression macro-F1: {f1_best:.3f}  (goal ≥ 0.625)\\n\")\n",
    "print(\"Classification Report (tuned):\\n\", classification_report(y_test, y_pred))\n"
   ],
   "id": "2c9af1f5bd6bf45b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best parameters: {'C': 0.1, 'class_weight': 'balanced', 'penalty': 'l2'}\n",
      "✅ Tuned Logistic Regression macro-F1: 0.800  (goal ≥ 0.625)\n",
      "\n",
      "Classification Report (tuned):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       148\n",
      "           1       0.72      0.66      0.69       146\n",
      "           2       0.76      0.75      0.76       146\n",
      "           3       0.90      0.91      0.90       146\n",
      "\n",
      "    accuracy                           0.80       586\n",
      "   macro avg       0.80      0.80      0.80       586\n",
      "weighted avg       0.80      0.80      0.80       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:37:29.072948Z",
     "start_time": "2025-04-26T15:37:29.019646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Task 2.3 — Evaluate Tuned Logistic Regression (F1 + Log-Loss)\n",
    "\n",
    "from sklearn.metrics import f1_score, log_loss, classification_report\n",
    "\n",
    "# 1) Predict classes and probabilities with the tuned model\n",
    "y_pred  = best_lr.predict(X_test)\n",
    "y_proba = best_lr.predict_proba(X_test)\n",
    "\n",
    "# 2) Compute macro-F1 (should match your ~0.80 from Task 2.2)\n",
    "f1    = f1_score(y_test, y_pred, average='macro')\n",
    "# 3) Compute multiclass log‐loss\n",
    "ll    = log_loss(y_test, y_proba)\n",
    "\n",
    "# 4) Report both\n",
    "print(f\"✅ Tuned Logistic Regression macro-F1: {f1:.3f}\")\n",
    "print(f\"📉 Tuned Logistic Regression log-loss: {ll:.3f}\\n\")\n",
    "print(\"Classification Report (Tuned):\\n\", classification_report(y_test, y_pred))\n"
   ],
   "id": "a11fa309f948f991",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tuned Logistic Regression macro-F1: 0.800\n",
      "📉 Tuned Logistic Regression log-loss: 0.540\n",
      "\n",
      "Classification Report (Tuned):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       148\n",
      "           1       0.72      0.66      0.69       146\n",
      "           2       0.76      0.75      0.76       146\n",
      "           3       0.90      0.91      0.90       146\n",
      "\n",
      "    accuracy                           0.80       586\n",
      "   macro avg       0.80      0.80      0.80       586\n",
      "weighted avg       0.80      0.80      0.80       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:58:36.130038Z",
     "start_time": "2025-04-26T15:58:29.985994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Task 3 — Supress warnings & hyper-tune KNN for max macro-F1\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # silence Loky/user warnings\n",
    "\n",
    "from sklearn.neighbors      import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics        import f1_score, classification_report\n",
    "\n",
    "# Expanded grid: try different k’s, both uniform/distance, Minkowski p=1 or 2\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'weights':     ['uniform', 'distance'],\n",
    "    'p':           [1, 2]\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid,\n",
    "    cv=3,              # 3-fold CV to speed up\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=1,          # avoid Loky warnings by not parallelizing\n",
    "    verbose=0\n",
    ")\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "# Grab the best model and evaluate on the test set\n",
    "best_knn  = grid_knn.best_estimator_\n",
    "best_params = grid_knn.best_params_\n",
    "y_pred     = best_knn.predict(X_test)\n",
    "f1_best    = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"🏆 Best KNN params: {best_params}\")\n",
    "print(f\"✅ Tuned KNN macro-F1: {f1_best:.3f}  (goal ≥ 0.625)\\n\")\n",
    "print(\"Classification Report (Tuned KNN):\\n\", classification_report(y_test, y_pred))\n"
   ],
   "id": "6d3d13d506f45f2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best KNN params: {'n_neighbors': 9, 'p': 2, 'weights': 'distance'}\n",
      "✅ Tuned KNN macro-F1: 0.738  (goal ≥ 0.625)\n",
      "\n",
      "Classification Report (Tuned KNN):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79       148\n",
      "           1       0.62      0.62      0.62       146\n",
      "           2       0.68      0.71      0.69       146\n",
      "           3       0.91      0.79      0.85       146\n",
      "\n",
      "    accuracy                           0.74       586\n",
      "   macro avg       0.74      0.74      0.74       586\n",
      "weighted avg       0.74      0.74      0.74       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T16:00:49.550581Z",
     "start_time": "2025-04-26T16:00:48.832659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Task 4.1 — Multiclass Decision Tree (baseline)\n",
    "\"\"\"\n",
    "Train a basic Decision Tree on your fully-preprocessed features,\n",
    "then evaluate its macro-F1 on the held-out test set.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# 1) Instantiate & train the tree\n",
    "dt_baseline = DecisionTreeClassifier(random_state=42)\n",
    "dt_baseline.fit(X_train, y_train)\n",
    "\n",
    "# 2) Predict & compute macro-F1\n",
    "y_pred_dt   = dt_baseline.predict(X_test)\n",
    "f1_dt       = f1_score(y_test, y_pred_dt, average='macro')\n",
    "\n",
    "print(f\"✅ Decision Tree (baseline) macro-F1: {f1_dt:.3f}  (goal ≥ 0.625)\\n\")\n",
    "print(\"Classification Report (baseline DT):\\n\", classification_report(y_test, y_pred_dt))\n"
   ],
   "id": "579321ce4355d74f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Decision Tree (baseline) macro-F1: 0.697  (goal ≥ 0.625)\n",
      "\n",
      "Classification Report (baseline DT):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       148\n",
      "           1       0.58      0.62      0.60       146\n",
      "           2       0.63      0.62      0.63       146\n",
      "           3       0.83      0.82      0.83       146\n",
      "\n",
      "    accuracy                           0.70       586\n",
      "   macro avg       0.70      0.70      0.70       586\n",
      "weighted avg       0.70      0.70      0.70       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T16:02:39.856797Z",
     "start_time": "2025-04-26T16:02:39.222751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Task 4.2 — Randomized tuning for Decision Tree to maximize macro-F1\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # quiet any loky / deprecation noise\n",
    "\n",
    "from sklearn.tree             import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics         import f1_score, classification_report\n",
    "from scipy.stats             import randint\n",
    "\n",
    "# Base classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Parameter distribution\n",
    "param_dist = {\n",
    "    'criterion':          ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth':          [None, 5, 10, 15, 20, 30],\n",
    "    'min_samples_split':  randint(2, 20),\n",
    "    'min_samples_leaf':   randint(1, 10),\n",
    "    'class_weight':       [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Randomized search (20 candidates, 5-fold CV)\n",
    "rand_dt = RandomizedSearchCV(\n",
    "    dt,\n",
    "    param_dist,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "rand_dt.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate best model\n",
    "best_dt     = rand_dt.best_estimator_\n",
    "best_params = rand_dt.best_params_\n",
    "y_pred      = best_dt.predict(X_test)\n",
    "f1_best     = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"🏆 Best DT params: {best_params}\")\n",
    "print(f\"✅ Tuned Decision Tree macro-F1: {f1_best:.3f}  (goal ≥ 0.625)\\n\")\n",
    "print(\"Classification Report (tuned DT):\\n\", classification_report(y_test, y_pred))\n"
   ],
   "id": "3fb0e7d611bbe3d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best DT params: {'class_weight': None, 'criterion': 'log_loss', 'max_depth': 30, 'min_samples_leaf': 7, 'min_samples_split': 9}\n",
      "✅ Tuned Decision Tree macro-F1: 0.718  (goal ≥ 0.625)\n",
      "\n",
      "Classification Report (tuned DT):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76       148\n",
      "           1       0.61      0.59      0.60       146\n",
      "           2       0.67      0.70      0.68       146\n",
      "           3       0.84      0.82      0.83       146\n",
      "\n",
      "    accuracy                           0.72       586\n",
      "   macro avg       0.72      0.72      0.72       586\n",
      "weighted avg       0.72      0.72      0.72       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T16:03:33.372820Z",
     "start_time": "2025-04-26T16:03:32.651919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Task 5.1 — XGBoost (baseline)\n",
    "\"\"\"\n",
    "Train a basic XGBClassifier on the preprocessed features,\n",
    "then evaluate its macro-F1 on the test set.\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# 1) Instantiate & train\n",
    "xgb = XGBClassifier(\n",
    "    use_label_encoder=False,    # suppress deprecation warning\n",
    "    eval_metric='mlogloss',     # multiclass logloss\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 2) Predict & evaluate\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "f1_xgb     = f1_score(y_test, y_pred_xgb, average='macro')\n",
    "print(f\"✅ XGBoost macro-F1: {f1_xgb:.3f}  (goal ≥ 0.625)\\n\")\n",
    "print(\"Classification Report (XGBoost):\\n\", classification_report(y_test, y_pred_xgb))"
   ],
   "id": "e6c8b45ec3c8f772",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost macro-F1: 0.792  (goal ≥ 0.625)\n",
      "\n",
      "Classification Report (XGBoost):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       148\n",
      "           1       0.68      0.67      0.67       146\n",
      "           2       0.76      0.77      0.77       146\n",
      "           3       0.89      0.90      0.89       146\n",
      "\n",
      "    accuracy                           0.79       586\n",
      "   macro avg       0.79      0.79      0.79       586\n",
      "weighted avg       0.79      0.79      0.79       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T16:04:29.407426Z",
     "start_time": "2025-04-26T16:04:28.514817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Task 5.2 — LightGBM (baseline)\n",
    "\"\"\"\n",
    "Train a basic LGBMClassifier on the preprocessed features,\n",
    "then evaluate its macro-F1 on the test set.\n",
    "\"\"\"\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# 1) Instantiate & train\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "lgb.fit(X_train, y_train)\n",
    "\n",
    "# 2) Predict & evaluate\n",
    "y_pred_lgb = lgb.predict(X_test)\n",
    "f1_lgb     = f1_score(y_test, y_pred_lgb, average='macro')\n",
    "print(f\"✅ LightGBM macro-F1: {f1_lgb:.3f}  (goal ≥ 0.625)\\n\")\n",
    "print(\"Classification Report (LightGBM):\\n\", classification_report(y_test, y_pred_lgb))\n"
   ],
   "id": "3d8003928dc220ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4320\n",
      "[LightGBM] [Info] Number of data points in the train set: 2344, number of used features: 222\n",
      "[LightGBM] [Info] Start training from score -1.377798\n",
      "[LightGBM] [Info] Start training from score -1.393144\n",
      "[LightGBM] [Info] Start training from score -1.386294\n",
      "[LightGBM] [Info] Start training from score -1.388002\n",
      "✅ LightGBM macro-F1: 0.794  (goal ≥ 0.625)\n",
      "\n",
      "Classification Report (LightGBM):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       148\n",
      "           1       0.68      0.71      0.69       146\n",
      "           2       0.77      0.76      0.76       146\n",
      "           3       0.89      0.88      0.89       146\n",
      "\n",
      "    accuracy                           0.79       586\n",
      "   macro avg       0.79      0.79      0.79       586\n",
      "weighted avg       0.79      0.79      0.79       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T16:08:52.428699Z",
     "start_time": "2025-04-26T16:08:51.297156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Task 5.3 — AdaBoost (baseline)\n",
    "\"\"\"\n",
    "Train a basic AdaBoostClassifier on the preprocessed features,\n",
    "then evaluate its macro-F1 on the test set.\n",
    "\"\"\"\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics    import f1_score, classification_report\n",
    "\n",
    "# 1) Instantiate & train\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# 2) Predict & evaluate\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "f1_ada     = f1_score(y_test, y_pred_ada, average='macro')\n",
    "print(f\"✅ AdaBoost macro-F1: {f1_ada:.3f}  (goal ≥ 0.625)\\n\")\n",
    "print(\"Classification Report (AdaBoost):\\n\", classification_report(y_test, y_pred_ada))\n"
   ],
   "id": "456734ee8eed0a3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AdaBoost macro-F1: 0.665  (goal ≥ 0.625)\n",
      "\n",
      "Classification Report (AdaBoost):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.64      0.70       148\n",
      "           1       0.54      0.65      0.59       146\n",
      "           2       0.57      0.74      0.64       146\n",
      "           3       0.90      0.61      0.73       146\n",
      "\n",
      "    accuracy                           0.66       586\n",
      "   macro avg       0.70      0.66      0.66       586\n",
      "weighted avg       0.70      0.66      0.66       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T16:08:13.889393Z",
     "start_time": "2025-04-26T16:07:07.969923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Task 5.3b — Tune AdaBoost for macro-F1 (fixed estimator argument)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble        import AdaBoostClassifier\n",
    "from sklearn.tree            import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics         import f1_score, classification_report\n",
    "\n",
    "# 1) Tunable base estimator\n",
    "base_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Note: modern sklearn uses `estimator=` instead of `base_estimator=`\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=base_tree,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2) Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators':            [50, 100, 200, 500],\n",
    "    'learning_rate':           [0.01, 0.1, 0.5, 1.0],\n",
    "    'estimator__max_depth':    [1, 2, 3]\n",
    "}\n",
    "\n",
    "# 3) GridSearchCV (3-fold CV, macro-F1)\n",
    "grid_ada = GridSearchCV(\n",
    "    ada,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_ada.fit(X_train, y_train)\n",
    "\n",
    "# 4) Evaluate best model\n",
    "best_ada    = grid_ada.best_estimator_\n",
    "best_params = grid_ada.best_params_\n",
    "y_pred_ada  = best_ada.predict(X_test)\n",
    "f1_ada      = f1_score(y_test, y_pred_ada, average='macro')\n",
    "\n",
    "print(f\"🏆 Best AdaBoost params: {best_params}\")\n",
    "print(f\"✅ Tuned AdaBoost macro-F1: {f1_ada:.3f}  (goal ≥ 0.625)\\n\")\n",
    "print(\"Classification Report (Tuned AdaBoost):\\n\", classification_report(y_test, y_pred_ada))\n"
   ],
   "id": "eeef4be0259e90ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best AdaBoost params: {'estimator__max_depth': 3, 'learning_rate': 0.1, 'n_estimators': 500}\n",
      "✅ Tuned AdaBoost macro-F1: 0.791  (goal ≥ 0.625)\n",
      "\n",
      "Classification Report (Tuned AdaBoost):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.76      0.80       148\n",
      "           1       0.64      0.73      0.69       146\n",
      "           2       0.76      0.79      0.77       146\n",
      "           3       0.93      0.88      0.90       146\n",
      "\n",
      "    accuracy                           0.79       586\n",
      "   macro avg       0.80      0.79      0.79       586\n",
      "weighted avg       0.80      0.79      0.79       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T16:05:04.715921Z",
     "start_time": "2025-04-26T16:04:56.233279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Task 5.4 — Grid Search to Tune XGBoost\n",
    "\"\"\"\n",
    "Grid-search key XGBoost hyperparameters (n_estimators, max_depth, learning_rate)\n",
    "to maximize macro-F1, then evaluate the best model on the test set.\n",
    "\"\"\"\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics        import f1_score, classification_report\n",
    "\n",
    "# 1) Define grid\n",
    "param_grid = {\n",
    "    'n_estimators':   [100, 200],\n",
    "    'max_depth':      [3, 5, 7],\n",
    "    'learning_rate':  [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# 2) Grid search (3-fold CV, optimizing macro-F1)\n",
    "grid_xgb = GridSearchCV(\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 3) Extract best model\n",
    "best_xgb     = grid_xgb.best_estimator_\n",
    "best_params  = grid_xgb.best_params_\n",
    "\n",
    "# 4) Predict & evaluate\n",
    "y_pred_best  = best_xgb.predict(X_test)\n",
    "f1_best      = f1_score(y_test, y_pred_best, average='macro')\n",
    "\n",
    "print(f\"🏆 Best XGBoost params: {best_params}\")\n",
    "print(f\"✅ Tuned XGBoost macro-F1: {f1_best:.3f}  (goal ≥ 0.625)\\n\")\n",
    "print(\"Classification Report (Tuned XGBoost):\\n\", classification_report(y_test, y_pred_best))\n"
   ],
   "id": "27f8086cf410e09a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best XGBoost params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "✅ Tuned XGBoost macro-F1: 0.800  (goal ≥ 0.625)\n",
      "\n",
      "Classification Report (Tuned XGBoost):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85       148\n",
      "           1       0.69      0.71      0.70       146\n",
      "           2       0.76      0.75      0.76       146\n",
      "           3       0.89      0.90      0.89       146\n",
      "\n",
      "    accuracy                           0.80       586\n",
      "   macro avg       0.80      0.80      0.80       586\n",
      "weighted avg       0.80      0.80      0.80       586\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### I applied GridSearch on AdaBoost and after one minute the resultant F1-score has been risen up from 0.665 to 0.791",
   "id": "34c52417f51b82f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 6: Extending KNN and Decision Trees to Multi-Label Classification\n",
    "\n",
    "In a **multi-label** setting, each instance can belong to *zero, one, or multiple* classes simultaneously (e.g. a house might be categorized as both “luxury” *and* “vintage”). To adapt our KNN and Decision-Tree pipelines:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. KNN → ML-KNN (Multi-Label KNN)\n",
    "\n",
    "- **Idea:** Instead of a single `y` per sample, we have a boolean vector `y ∈ {0,1}^L` for L labels.\n",
    "- **Algorithm (ML-KNN):**\n",
    "  1. For each test point, find its *k* nearest neighbors in feature space.\n",
    "  2. Count how many of those neighbors have each label *j* (call that count \\( c_j \\)).\n",
    "  3. Use a Bayesian update (with smoothing) to estimate\n",
    "     \\[\n",
    "       P(\\text{label}_j = 1 \\mid c_j)\n",
    "     \\]\n",
    "     and threshold these probabilities to decide which labels to assign.\n",
    "- **Implementation in scikit-learn:**\n",
    "  ```python\n",
    "  from skmultilearn.adapt import MLkNN\n",
    "  knn_ml = MLkNN(k=5)\n",
    "  knn_ml.fit(X_train, Y_train_multi)      # Y_train_multi: shape (n_samples, n_labels)\n",
    "  Y_pred       = knn_ml.predict(X_test)\n"
   ],
   "id": "fb1b4162b7c2ccef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
